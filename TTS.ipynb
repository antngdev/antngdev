{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antngdev/antngdev/blob/main/TTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv6wKD48of3x"
      },
      "source": [
        "# ğŸ”¥ğŸ”¥ğŸ”¥**viXTTS Demo**ğŸ—£ï¸ğŸ—£ï¸ğŸ—£ï¸\n",
        "\n",
        "Demo nÃ y giÃºp báº¡n cháº¡y viXTTS miá»…n phÃ­ trÃªn Google Colab!\n",
        "Xem thÃ´ng tin mÃ´ hÃ¬nh táº¡i [Ä‘Ã¢y](https://huggingface.co/capleaf/viXTTS)\n",
        "\n",
        "Báº¡n cÃ³ thá»ƒ dÃ¹ng demo nÃ y vá»›i má»¥c Ä‘Ã­ch:\n",
        "- âœ…Má»¥c Ä‘Ã­ch cÃ¡ nhÃ¢n, há»c táº­p, nghiÃªn cá»©u, thá»­ nghiá»‡m\n",
        "\n",
        "Báº¡n **KHÃ”NG** Ä‘Æ°á»£c dÃ¹ng demo nÃ y vá»›i má»¥c Ä‘Ã­ch:\n",
        "- âŒMá»¥c Ä‘Ã­ch trÃ¡i Ä‘áº¡o Ä‘á»©c, vi pháº¡m phÃ¡p luáº­t Viá»‡t Nam\n",
        "- âŒTáº¡o ra ná»™i dung gÃ¢y thÃ¹ ghÃ©t, ká»³ thá»‹, báº¡o lá»±c hoáº·c ná»™i dung vi pháº¡m báº£n quyá»n\n",
        "- âŒGiáº£ máº¡o danh tÃ­nh hoáº·c gÃ¢y hiá»ƒu nháº§m ráº±ng ná»™i dung Ä‘Æ°á»£c táº¡o ra bá»Ÿi má»™t cÃ¡ nhÃ¢n hoáº·c tá»• chá»©c khÃ¡c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kIEFgM3gnEZm",
        "outputId": "8908c7d1-17a2-43eb-9332-ff93d617a893",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 22 04:28:49 PM +07 2024\n",
            " > CÃ i Ä‘áº·t thÆ° viá»‡n...\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m834.7/834.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for TTS (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's legacy dependency resolver does not consider dependency conflicts when selecting packages. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.16 requires numpy>=1.24, but you'll have numpy 1.22.0 which is incompatible.\n",
            "albumentations 1.4.15 requires numpy>=1.24.4, but you'll have numpy 1.22.0 which is incompatible.\n",
            "arviz 0.19.0 requires numpy>=1.23.0, but you'll have numpy 1.22.0 which is incompatible.\n",
            "astropy 6.1.4 requires numpy>=1.23, but you'll have numpy 1.22.0 which is incompatible.\n",
            "bigframes 1.22.0 requires numpy>=1.24.0, but you'll have numpy 1.22.0 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you'll have numpy 1.22.0 which is incompatible.\n",
            "contourpy 1.3.0 requires numpy>=1.23, but you'll have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires numpy<3.0a0,>=1.23, but you'll have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you'll have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you'll have pandas 1.5.3 which is incompatible.\n",
            "ibis-framework 9.2.0 requires numpy<3,>=1.23.2, but you'll have numpy 1.22.0 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you'll have numpy 1.22.0 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you'll have numpy 1.22.0 which is incompatible.\n",
            "librosa 0.10.2.post1 requires numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3, but you'll have numpy 1.22.0 which is incompatible.\n",
            "mizani 0.11.4 requires numpy>=1.23.0, but you'll have numpy 1.22.0 which is incompatible.\n",
            "mizani 0.11.4 requires pandas>=2.1.0, but you'll have pandas 1.5.3 which is incompatible.\n",
            "numexpr 2.10.1 requires numpy>=1.23.0, but you'll have numpy 1.22.0 which is incompatible.\n",
            "nx-cugraph-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you'll have numpy 1.22.0 which is incompatible.\n",
            "pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you'll have numpy 1.22.0 which is incompatible.\n",
            "plotnine 0.13.6 requires numpy>=1.23.0, but you'll have numpy 1.22.0 which is incompatible.\n",
            "plotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you'll have pandas 1.5.3 which is incompatible.\n",
            "pylibraft-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you'll have numpy 1.22.0 which is incompatible.\n",
            "rmm-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you'll have numpy 1.22.0 which is incompatible.\n",
            "scikit-image 0.24.0 requires numpy>=1.23, but you'll have numpy 1.22.0 which is incompatible.\n",
            "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you'll have numpy 1.22.0 which is incompatible.\n",
            "statsmodels 0.14.4 requires numpy<3,>=1.22.3, but you'll have numpy 1.22.0 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you'll have numpy 1.22.0 which is incompatible.\n",
            "xarray 2024.9.0 requires numpy>=1.24, but you'll have numpy 1.22.0 which is incompatible.\n",
            "xarray 2024.9.0 requires pandas>=2.1, but you'll have pandas 1.5.3 which is incompatible.\n",
            "xarray-einstats 0.8.0 requires numpy>=1.23, but you'll have numpy 1.22.0 which is incompatible.\n",
            "gruut 2.2.3 requires networkx<3.0.0,>=2.5.0, but you'll have networkx 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.4/40.4 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m600.9/600.9 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for cutlet (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for unidic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 2.1.2 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.1.2 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.2 which is incompatible.\n",
            "gruut 2.2.3 requires networkx<3.0.0,>=2.5.0, but you have networkx 3.4.1 which is incompatible.\n",
            "gruut 2.2.3 requires numpy<2.0.0,>=1.19.0, but you have numpy 2.1.2 which is incompatible.\n",
            "mizani 0.11.4 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.2 which is incompatible.\n",
            "plotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.5.3 which is incompatible.\n",
            "pytensor 2.25.5 requires numpy<2,>=1.17.0, but you have numpy 2.1.2 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.1.2 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.2 which is incompatible.\n",
            "xarray 2024.9.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.2/113.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m568.8/568.8 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 1.26.4 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "gruut 2.2.3 requires networkx<3.0.0,>=2.5.0, but you have networkx 3.4.1 which is incompatible.\n",
            "mizani 0.11.4 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2024.9.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m > Táº£i mÃ´ hÃ¬nh...\n"
          ]
        }
      ],
      "source": [
        "# @title 1. âš™ï¸ **CÃ i Ä‘áº·t**\n",
        "# @markdown ğŸ‘ˆNháº¥n nÃºt nÃ y Ä‘á»ƒ cÃ i Ä‘áº·t (~5 phÃºt)\n",
        "# Change timezone to Vietnam\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Asia/Ho_Chi_Minh /etc/localtime\n",
        "!date\n",
        "\n",
        "print(\" > CÃ i Ä‘áº·t thÆ° viá»‡n...\")\n",
        "!rm -rf TTS/\n",
        "!git clone --branch add-vietnamese-xtts -q https://github.com/thinhlpg/TTS.git\n",
        "!pip install --use-deprecated=legacy-resolver -q -e TTS\n",
        "!pip install deepspeed -q\n",
        "!pip install -q vinorm==2.0.7\n",
        "!pip install -q cutlet\n",
        "!pip install -q unidic==1.1.0\n",
        "!pip install -q underthesea\n",
        "!pip install -q gradio==4.35\n",
        "!pip install deepfilternet==0.5.6 -q\n",
        "\n",
        "import os\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "\n",
        "os.system(\"python -m unidic download\")\n",
        "print(\" > Táº£i mÃ´ hÃ¬nh...\")\n",
        "snapshot_download(repo_id=\"thinhlpg/viXTTS\",\n",
        "                  repo_type=\"model\",\n",
        "                  local_dir=\"model\")\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\" > âœ… CÃ i Ä‘áº·t hoÃ n táº¥t, báº¡n hÃ£y cháº¡y tiáº¿p cÃ¡c bÆ°á»›c tiáº¿p theo nhÃ©!\")\n",
        "quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGSVcv3xuWWi"
      },
      "outputs": [],
      "source": [
        "# The inference code is adopted from https://github.com/coqui-ai/TTS/blob/dev/TTS/demos/xtts_ft_demo/xtts_demo.py\n",
        "# @title 2. ğŸ¤— **Sá»­ dá»¥ng**\n",
        "# @markdown ğŸ‘ˆ Nháº¥n Ä‘á»ƒ cháº¡y.\n",
        "# @markdown Náº¿u gáº·p lá»—i thÃ¬ cÅ©ng nháº¥n nÃºt nÃ y nhÃ©!\n",
        "\n",
        "# @markdown Láº§n Ä‘áº§u cháº¡y sáº½ hÆ¡i lÃ¢u, báº¡n chá» tÃ­ nhÃ©!\n",
        "\n",
        "# @markdown Káº¿t quáº£ sáº½ Ä‘Æ°á»£c lÆ°u vÃ o `/content/output`\n",
        "\n",
        "# @markdown Chá»n ngÃ´n ngá»¯:\n",
        "language = \"Tiáº¿ng Viá»‡t\" # @param [\"Tiáº¿ng Viá»‡t\", \"Tiáº¿ng Anh\",\"Tiáº¿ng TÃ¢y Ban Nha\", \"Tiáº¿ng PhÃ¡p\",\"Tiáº¿ng Äá»©c\",\"Tiáº¿ng Ã\", \"Tiáº¿ng Bá»“ ÄÃ o Nha\", \"Tiáº¿ng Ba Lan\", \"Tiáº¿ng Thá»• NhÄ© Ká»³\", \"Tiáº¿ng Nga\", \"Tiáº¿ng HÃ  Lan\", \"Tiáº¿ng SÃ©c\", \"Tiáº¿ng áº¢ Ráº­p\", \"Tiáº¿ng Trung (giáº£n thá»ƒ)\", \"Tiáº¿ng Nháº­t\", \"Tiáº¿ng Hungary\", \"Tiáº¿ng HÃ n\", \"Tiáº¿ng Hindi\"]\n",
        "# @markdown VÄƒn báº£n Ä‘á»ƒ Ä‘á»c. Äá»™ dÃ i tá»‘i thiá»ƒu má»—i cÃ¢u nÃªn tá»« 10 tá»« Ä‘á»ƒ Ä‘áº·t káº¿t quáº£ tá»‘t nháº¥t.\n",
        "input_text =\"Xin chÃ o, tÃ´i lÃ  má»™t cÃ´ng cá»¥ cÃ³ kháº£ nÄƒng chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh giá»ng nÃ³i tá»± nhiÃªn, Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi nhÃ³m NÃ³n lÃ¡. TÃ´i cÃ³ thá»ƒ há»• trá»£ ngÆ°á»i khiáº¿m thá»‹,  Ä‘á»c sÃ¡ch nÃ³i, lÃ m trá»£ lÃ½ áº£o, review phim, lÃ m waifu Ä‘á»ƒ an á»§i báº¡n, vÃ  phá»¥c vá»¥ nhiá»u má»¥c Ä‘Ã­ch khÃ¡c.\" # @param {type:\"string\"}\n",
        "# @markdown Chá»n giá»ng máº«u:\n",
        "reference_audio = \"model/vi_sample.wav\" # @param [ \"model/user_sample.wav\",  \"model/vi_sample.wav\",  \"model/samples/nam-calm.wav\",  \"model/samples/nam-cham.wav\",  \"model/samples/nam-nhanh.wav\",  \"model/samples/nam-truyen-cam.wav\",  \"model/samples/nu-calm.wav\",  \"model/samples/nu-cham.wav\",  \"model/samples/nu-luu-loat.wav\",  \"model/samples/nu-nhan-nha.wav\",  \"model/samples/nu-nhe-nhang.wav\"]\n",
        "# @markdown Tá»± Ä‘á»™ng chuáº©n hÃ³a chá»¯ (VD: 20/11 -> hai mÆ°Æ¡i thÃ¡ng mÆ°á»i má»™t)\n",
        "normalize_text = True # @param {type:\"boolean\"}\n",
        "# @markdown In chi tiáº¿t xá»­ lÃ½\n",
        "verbose = True # @param {type:\"boolean\"}\n",
        "# @markdown LÆ°u tá»«ng cÃ¢u thÃ nh file riÃªng láº».\n",
        "output_chunks = True # @param {type:\"boolean\"}\n",
        "\n",
        "from IPython.display import clear_output\n",
        "def cry_and_quit():\n",
        "    clear_output()\n",
        "    print(\"> Lá»—i rá»“i huhu ğŸ˜­ğŸ˜­, báº¡n hÃ£y nháº¥n cháº¡y láº¡i pháº§n nÃ y nhÃ©!\")\n",
        "    quit()\n",
        "\n",
        "import os\n",
        "import string\n",
        "import unicodedata\n",
        "from datetime import datetime\n",
        "from pprint import pprint\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from tqdm import tqdm\n",
        "from underthesea import sent_tokenize\n",
        "from unidecode import unidecode\n",
        "\n",
        "try:\n",
        "    from vinorm import TTSnorm\n",
        "    from TTS.tts.configs.xtts_config import XttsConfig\n",
        "    from TTS.tts.models.xtts import Xtts\n",
        "except:\n",
        "    cry_and_quit()\n",
        "\n",
        "# Load model\n",
        "def clear_gpu_cache():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def load_model(xtts_checkpoint, xtts_config, xtts_vocab):\n",
        "    clear_gpu_cache()\n",
        "    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n",
        "        return \"You need to run the previous steps or manually set the `XTTS checkpoint path`, `XTTS config path`, and `XTTS vocab path` fields !!\"\n",
        "    config = XttsConfig()\n",
        "    config.load_json(xtts_config)\n",
        "    XTTS_MODEL = Xtts.init_from_config(config)\n",
        "    print(\"Loading XTTS model! \")\n",
        "    XTTS_MODEL.load_checkpoint(config,\n",
        "                               checkpoint_path=xtts_checkpoint,\n",
        "                               vocab_path=xtts_vocab,\n",
        "                               use_deepspeed=True)\n",
        "    if torch.cuda.is_available():\n",
        "        XTTS_MODEL.cuda()\n",
        "\n",
        "    print(\"Model Loaded!\")\n",
        "    return XTTS_MODEL\n",
        "\n",
        "\n",
        "def get_file_name(text, max_char=50):\n",
        "    filename = text[:max_char]\n",
        "    filename = filename.lower()\n",
        "    filename = filename.replace(\" \", \"_\")\n",
        "    filename = filename.translate(str.maketrans(\"\", \"\", string.punctuation.replace(\"_\", \"\")))\n",
        "    filename = unidecode(filename)\n",
        "    current_datetime = datetime.now().strftime(\"%m%d%H%M%S\")\n",
        "    filename = f\"{current_datetime}_{filename}\"\n",
        "    return filename\n",
        "\n",
        "\n",
        "def calculate_keep_len(text, lang):\n",
        "    if lang in [\"ja\", \"zh-cn\"]:\n",
        "        return -1\n",
        "\n",
        "    word_count = len(text.split())\n",
        "    num_punct = (\n",
        "        text.count(\".\")\n",
        "        + text.count(\"!\")\n",
        "        + text.count(\"?\")\n",
        "        + text.count(\",\")\n",
        "    )\n",
        "\n",
        "    if word_count < 5:\n",
        "        return 15000 * word_count + 2000 * num_punct\n",
        "    elif word_count < 10:\n",
        "        return 13000 * word_count + 2000 * num_punct\n",
        "    return -1\n",
        "\n",
        "\n",
        "def normalize_vietnamese_text(text):\n",
        "    text = (\n",
        "        TTSnorm(text, unknown=False, lower=False, rule=True)\n",
        "        .replace(\"..\", \".\")\n",
        "        .replace(\"!.\", \"!\")\n",
        "        .replace(\"?.\", \"?\")\n",
        "        .replace(\" .\", \".\")\n",
        "        .replace(\" ,\", \",\")\n",
        "        .replace('\"', \"\")\n",
        "        .replace(\"'\", \"\")\n",
        "        .replace(\"AI\", \"Ã‚y Ai\")\n",
        "        .replace(\"A.I\", \"Ã‚y Ai\")\n",
        "    )\n",
        "    return text\n",
        "\n",
        "\n",
        "def run_tts(XTTS_MODEL, lang, tts_text, speaker_audio_file,\n",
        "            normalize_text= True,\n",
        "            verbose=False,\n",
        "            output_chunks=False):\n",
        "    \"\"\"\n",
        "    Run text-to-speech (TTS) synthesis using the provided XTTS_MODEL.\n",
        "\n",
        "    Args:\n",
        "        XTTS_MODEL: A pre-trained TTS model.\n",
        "        lang (str): The language of the input text.\n",
        "        tts_text (str): The text to be synthesized into speech.\n",
        "        speaker_audio_file (str): Path to the audio file of the speaker to condition the synthesis on.\n",
        "        normalize_text (bool, optional): Whether to normalize the input text. Defaults to True.\n",
        "        verbose (bool, optional): Whether to print verbose information. Defaults to False.\n",
        "        output_chunks (bool, optional): Whether to save synthesized speech chunks separately. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the synthesized audio file.\n",
        "    \"\"\"\n",
        "\n",
        "    if XTTS_MODEL is None or not speaker_audio_file:\n",
        "        return \"You need to run the previous step to load the model !!\", None, None\n",
        "\n",
        "    output_dir = \"./output\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    gpt_cond_latent, speaker_embedding = XTTS_MODEL.get_conditioning_latents(\n",
        "        audio_path=speaker_audio_file,\n",
        "        gpt_cond_len=XTTS_MODEL.config.gpt_cond_len,\n",
        "        max_ref_length=XTTS_MODEL.config.max_ref_len,\n",
        "        sound_norm_refs=XTTS_MODEL.config.sound_norm_refs,\n",
        "    )\n",
        "\n",
        "    if normalize_text and lang == \"vi\":\n",
        "        # Bug on google colab\n",
        "        try:\n",
        "            tts_text = normalize_vietnamese_text(tts_text)\n",
        "        except:\n",
        "            cry_and_quit()\n",
        "\n",
        "    if lang in [\"ja\", \"zh-cn\"]:\n",
        "        tts_texts = tts_text.split(\"ã€‚\")\n",
        "    else:\n",
        "        tts_texts = sent_tokenize(tts_text)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Text for TTS:\")\n",
        "        pprint(tts_texts)\n",
        "\n",
        "    wav_chunks = []\n",
        "    for text in tqdm(tts_texts):\n",
        "        if text.strip() == \"\":\n",
        "            continue\n",
        "\n",
        "        wav_chunk = XTTS_MODEL.inference(\n",
        "            text=text,\n",
        "            language=lang,\n",
        "            gpt_cond_latent=gpt_cond_latent,\n",
        "            speaker_embedding=speaker_embedding,\n",
        "            temperature=0.3,\n",
        "            length_penalty=1.0,\n",
        "            repetition_penalty=10.0,\n",
        "            top_k=30,\n",
        "            top_p=0.85,\n",
        "        )\n",
        "\n",
        "        # Quick hack for short sentences\n",
        "        keep_len = calculate_keep_len(text, lang)\n",
        "        wav_chunk[\"wav\"] = torch.tensor(wav_chunk[\"wav\"][:keep_len])\n",
        "\n",
        "        if output_chunks:\n",
        "            out_path = os.path.join(output_dir, f\"{get_file_name(text)}.wav\")\n",
        "            torchaudio.save(out_path, wav_chunk[\"wav\"].unsqueeze(0), 24000)\n",
        "            if verbose:\n",
        "                print(f\"Saved chunk to {out_path}\")\n",
        "\n",
        "        wav_chunks.append(wav_chunk[\"wav\"])\n",
        "\n",
        "    out_wav = torch.cat(wav_chunks, dim=0).unsqueeze(0)\n",
        "    out_path = os.path.join(output_dir, f\"{get_file_name(tts_text)}.wav\")\n",
        "    torchaudio.save(out_path, out_wav, 24000)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Saved final file to {out_path}\")\n",
        "\n",
        "    return out_path\n",
        "\n",
        "\n",
        "language_code_map = {\n",
        "    \"Tiáº¿ng Viá»‡t\": \"vi\",\n",
        "    \"Tiáº¿ng Anh\": \"en\",\n",
        "    \"Tiáº¿ng TÃ¢y Ban Nha\": \"es\",\n",
        "    \"Tiáº¿ng PhÃ¡p\": \"fr\",\n",
        "    \"Tiáº¿ng Äá»©c\": \"de\",\n",
        "    \"Tiáº¿ng Ã\": \"it\",\n",
        "    \"Tiáº¿ng Bá»“ ÄÃ o Nha\": \"pt\",\n",
        "    \"Tiáº¿ng Ba Lan\": \"pl\",\n",
        "    \"Tiáº¿ng Thá»• NhÄ© Ká»³\": \"tr\",\n",
        "    \"Tiáº¿ng Nga\": \"ru\",\n",
        "    \"Tiáº¿ng HÃ  Lan\": \"nl\",\n",
        "    \"Tiáº¿ng SÃ©c\": \"cs\",\n",
        "    \"Tiáº¿ng áº¢ Ráº­p\": \"ar\",\n",
        "    \"Tiáº¿ng Trung (giáº£n thá»ƒ)\": \"zh-cn\",\n",
        "    \"Tiáº¿ng Nháº­t\": \"ja\",\n",
        "    \"Tiáº¿ng Hungary\": \"hu\",\n",
        "    \"Tiáº¿ng HÃ n\": \"ko\",\n",
        "    \"Tiáº¿ng Hindi\": \"hi\"\n",
        "}\n",
        "\n",
        "print(\"> Äang náº¡p mÃ´ hÃ¬nh...\")\n",
        "try:\n",
        "    if not vixtts_model:\n",
        "        vixtts_model = load_model(xtts_checkpoint=\"model/model.pth\",\n",
        "                                xtts_config=\"model/config.json\",\n",
        "                                xtts_vocab=\"model/vocab.json\")\n",
        "except:\n",
        "    vixtts_model = load_model(xtts_checkpoint=\"model/model.pth\",\n",
        "                                xtts_config=\"model/config.json\",\n",
        "                                xtts_vocab=\"model/vocab.json\")\n",
        "clear_output()\n",
        "print(\"> ÄÃ£ náº¡p mÃ´ hÃ¬nh\")\n",
        "\n",
        "if not os.path.exists(reference_audio):\n",
        "    print(\"âš ï¸âš ï¸âš ï¸Báº¡n chÆ°a táº£i file Ã¢m thanh lÃªn. HÃ£y chá»n giá»ng khÃ¡c, hoáº·c táº£i file cá»§a báº¡n lÃªn á»Ÿ bÃªn dÆ°á»›i.âš ï¸âš ï¸âš ï¸\")\n",
        "    audio_file=\"/content/model/vi_sample.wav\"\n",
        "else:\n",
        "    audio_file = run_tts(vixtts_model,\n",
        "            lang=language_code_map[language],\n",
        "            tts_text=input_text,\n",
        "            speaker_audio_file=reference_audio,\n",
        "            normalize_text=normalize_text,\n",
        "            verbose=verbose,\n",
        "            output_chunks=output_chunks,)\n",
        "\n",
        "from IPython.display import Audio\n",
        "Audio(audio_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C7Bm9c2iTmpQ"
      },
      "outputs": [],
      "source": [
        "# @title ğŸ¤ **Táº£i file Ã¢m thanh cá»§a báº¡n lÃªn**\n",
        "# @markdown Äá»ƒ Ä‘áº¡t cháº¥t lÆ°á»£ng tá»‘t nháº¥t, hÃ£y tham kháº£o file '/content/model/vi_sample.wav'\n",
        "# @\n",
        "import os\n",
        "import locale\n",
        "from google.colab import files\n",
        "\n",
        "denoise = True # @param {type:\"boolean\"}\n",
        "\n",
        "# Upload the audio file\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "    # Convert the audio file to WAV format using ffmpeg\n",
        "    uploaded_dir = os.path.dirname(filename)\n",
        "    if denoise:\n",
        "        !deepFilter \"{filename}\"\n",
        "        !ffmpeg -i \"{filename.replace('.wav', '_DeepFilterNet3.wav')}\" -ac 1 -ar 22050 -vn /content/model/user_sample.wav -y -hide_banner -loglevel error\n",
        "        os.remove(filename.replace('.wav', '_DeepFilterNet3.wav'))\n",
        "    else:\n",
        "        !ffmpeg -i \"{filename}\" -ac 1 -ar 22050 -vn /content/model/user_sample.wav -y -hide_banner -loglevel error\n",
        "        os.remove(filename)\n",
        "    break\n",
        "\n",
        "from IPython.display import Audio, clear_output\n",
        "clear_output()\n",
        "print(\"> ÄÃ£ táº£i file Ã¢m thanh lÃªn\")\n",
        "Audio(\"/content/model/user_sample.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6sC6v6pNQVt5"
      },
      "outputs": [],
      "source": [
        "# @title â¬ **LÆ°u káº¿t quáº£ vÃ o Drive**\n",
        "# @markdown Cháº¡y pháº§n nÃ y Ä‘á»ƒ lÆ°u káº¿t quáº£ vÃ o Google Drive cá»§a báº¡n\n",
        "# @markdown `/content/drive/MyDrive/vixtts-output`\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save the output folder in \"vixtts-output\" in Google Drive, without overwriting existing files\n",
        "!cp -n -r /content/output/* /content/drive/MyDrive/vixtts-output\n",
        "print(\"> ÄÃ£ lÆ°u káº¿t quáº£ vÃ o Google Drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7-X619YbX0n-"
      },
      "outputs": [],
      "source": [
        "# @title âš ï¸ **Dá»n káº¿t quáº£**\n",
        "# @markdown Cháº¡y pháº§n nÃ y Ä‘á»ƒ xÃ³a toÃ n bá»™ file trong `/content/output`\n",
        "import shutil\n",
        "shutil.rmtree('/content/output')\n",
        "print(\"ÄÃ£ xÃ³a toÃ n bá»™ file trong /content/output\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ğŸ“´ **Táº¯t Demo**\n",
        "# @markdown Khi cháº¡y xong thÃ¬ báº¡n hÃ£y táº¯t demo Ä‘á»ƒ tiáº¿t kiá»‡m GPU nhÃ©!\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cvzEJ8c_PkRb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}